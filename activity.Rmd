---
title: "Introduction to extreme value analysis in R"
output: html_notebook
editor_options:
  chunk_output_type: console
---

In September 2013, Boulder Creek experienced massive flooding following a prolonged bout of precipitation. 
Over a five day period, a storm dumped more precipitation in Boulder county than what is expected over the course of a typical year. 

![This is an animated loop of water vapor systems over the western area of North America on September 12th, 2013 as shown by the GOES-15 and GOES-13 satellites. The storm that caused the 2013 Colorado flooding was kept in a confined area over the Eastern Range of the Rocky Mountains in Colorado by these water vapor systems. Source: http://cimss.ssec.wisc.edu/goes/blog/archives/13876](https://upload.wikimedia.org/wikipedia/commons/9/97/North_American_Water_Vapor_Systems.gif)

# What is a 100 year flood?

The media reported the event as a 1,000 year rain and a 100 year flood. 
But, what do these values actually mean, and where did they come from?

#### Goals

This exercise is designed with the following goals in mind:

1. Understand **cumulative distribution functions**, **exceedance probabilities**, and **return intervals**
2. Get hands-on experience with extreme value analysis in R
3. Estimate return intervals for the 2013 floods

### Loading packages

We are going to use some R packages to help us acquire, process, and model
the flood data. 
If these aren't already installed, you may install them with the 
`install.packages` function (e.g., `install.packages('leaflet')`).

```{r, message=FALSE}
library(dataRetrieval)
library(tidyverse)
library(lubridate)
library(leaflet)
library(extRemes)
```

## Acquiring flood data

We are going to use the `dataRetrieval` R package to get some stream discharge
data 
This package is maintained by USGS and openly available on 
[GitHub](https://github.com/USGS-R/dataRetrieval) to the research community.

The USGS has stream gauges in and around Boulder, identified by site numbers. 
Here are some of them:

- 06730500
- 06730200
- 06729500

We can use the `whatNWISsites` function to get metadata on each of these sites:

```{r}
usgs_sites <- c('06730500', 
                '06730200', 
                '06729500')
site_info <- whatNWISsites(sites = usgs_sites)
site_info
```

Let's visualize the locations of these stream gauges to provide some context:

```{r}
leaflet() %>%
  addTiles() %>%
  addMarkers(lng = site_info$dec_long_va, 
             lat = site_info$dec_lat_va, 
             popup = paste(site_info$site_no, 
                           site_info$station_nm))
```

Now, we can use the `readNWISdata` function to fetch stream discharge data at 
these locations. This function may take a couple of minutes while to run. 

```{r}
discharge_data <- readNWISdata(siteNumbers = usgs_sites, 
                               parameterCd = '00060', # cubic ft. / sec
                               startDate = '1870-10-01',
                               endDate = '2018-01-01')
```

## Data cleaning

The data are in kind of a raw form - let's clean them up with an eye toward 
working with annual maxima. 
We are going to:

1. Rename columns to be more human readable
2. Create `year` and `DOY` (day of year) columns from the date field
3. Filter out suspect observations
4. Convert the `data.frame` to a `tibble`, which is easier to work with

```{r}
discharge_data <- discharge_data  %>%
  rename(quality_code = X_00060_00003_cd, 
         discharge_cfs = X_00060_00003) %>%
  mutate(year = year(dateTime)) %>%
  filter(quality_code == 'A') %>%
  as_tibble
```

Let's see what the data look like: 

```{r}
glimpse(discharge_data)
```

What do these discharge time series look like?

```{r}
discharge_data %>%
  ggplot(aes(dateTime, discharge_cfs, group = year)) + 
  geom_point(size = .1) + 
  facet_wrap(~ site_no, ncol = 1) + 
  xlab('Date') + 
  ylab('Discharge (CFS)')
```

Uh oh - looks like some of the data are missing! 
Wasn't this supposed to be an in-class activity? 
We are all adults here, and real data are messy.

Only two of the stations were operational during the flood: `06730500` and `06730200`. 
Which of these do you think will be most useful for computing annual return intervals? 

Here, I'll use station USGS-06730500, because it has a longer record than any 
other site, and contains data from the flood. 
This station is at the mouth of Boulder Creek, so I'll give it an informative 
name:

```{r}
boulder_creek_mouth <- discharge_data %>%
  filter(site_no == '06730500')
```

## Computing annual maxima

So, we know that we want to model annual maxima - but first we need to compute 
them. 
We can do this in two steps, first grouping by year, and second summarizing 
each year by its maximum. 

```{r}
annual_maxima <- boulder_creek_mouth %>%
  group_by(year) %>%
  summarize(max_cfs = max(discharge_cfs))
```

Below, generate a plot of maximum discharge at this site through time:

```{r}
# your code here
```

Is there anything that strikes you as unusual about this plot?

## Back to the question at hand

Clearly, the maximum flow in 2013 was quite a bit higher than anything seen 
previously in this record. 
The challenge now is to come up with an estimate for the return interval.
A **return interval** is a simple idea with a complex formal definition. 
Simply put, if you expect to see discharge of a certain value once every 10 
years, then the return interval for that level of discharge is 10 years. 
More formally, it is the inverse of the probability that an event with be 
exceeded in any one year. 
So, if the probability of exceeding a particular value in one year is 0.1, the
return interval is the inverse of 0.1 (1 / 0.1 = 10).
Return intervals are also called return periods, recurrence intervals, and 
repeat intervals. 

### How to find return intervals

#### A naiive approach

What if we used the empirical data alone to estimate a return interval? 
All we need is some probability of exceeding a value in one year, right?
Once we have that probability, we need to compute its inverse.
How hard could it be?

Let's ask the question: what's the return interval for discharge exceeding
1000 cubic feet per second?

First, we compute some estimate of the probability of exceeding 1000 cfs in a 
year. 
We could do this empirically by asking what fraction of years have maxima 
that exceed 1500 cfs:

```{r}
exceedance_fraction <- mean(annual_maxima$max_cfs > 1500)
exceedance_fraction
```

So, our estimated **exceedance probability** is 
`r exceedance_fraction`.
To transform this into a **return interval** we take the inverse:

```{r}
return_interval <- 1 / exceedance_fraction
return_interval
```

So, we might say that the return interval for 1500 cfs 
is $\approx$ `r return_interval`.

Now, find the return interval for 4000 cfs using the same approach:

```{r}
# your code here
```

#### A model-based approach

It turns out that we can improve our approach by applying some results from 
extreme value theory. 
Specifically, we expect that these annual maxima might be well-represented by 
a specific probability distribution: the Generalize Extreme Value (GEV) 
distribution. 

The GEV distribution has three parameters:

- location ($\mu$)
- scale ($\sigma > 0$)
- shape $\xi$

The cumulative probability density function (CDF) is given by:

$$F(x; \mu, \sigma, \xi) = \begin{cases}\exp(-(1+\xi (x-\mu)/\sigma)^{-1/\xi}) & \xi\neq0 \\ \exp(-\exp(-(x-\mu)/\sigma)) & \xi = 0\end{cases}$$

#### Wait, what's a CDF and what are all of those squiggly lines?

A cumulative probability density function gives the probability that a random 
variable is less than or equal to some value $x$. 
The notation $F(x; \mu, \sigma, \xi)$ represents the CDF of a 
distribution $F$, that has three parameters ($\mu, \sigma, \xi$). 
So you can read $F(x; \mu, \sigma, \xi)$ as the probability of a value 
less than or equal to $x$ for the distribution $F$, parameterized by all of the 
things after the colon.
The squiggly lines are compact representations of the English words
'location', 'scale', and 'shape'.

In order to use the GEV distribution, we need to estimate these three 
parameters: $\mu, \sigma, \xi$.

### Parameter estimation for the GEV distribution

We are going to use the `extRemes` package for parameter estimation. 
There are a lot of packages in R that can fit extreme value models, this is 
one of the more well-documented ones. 
For an overview of other options, check out the 
[CRAN task view page for extreme value analysis](https://cran.r-project.org/web/views/ExtremeValue.html)

The `extRemes` package has a function `fevd` to fit an extreme value 
distribution to data. 
There are three arguments that we'll need to specify: 

- `x`: a numeric vector of annual maxima
- `type`: the type of distribution (we need `type = 'GEV`)

Below, we use the `fevd` function to fit your model. 

```{r}
model <- fevd(x = annual_maxima$max_cfs, type = 'GEV')
```


*Hint*

- Check out `?fevd` to see the functions help file


Once your model fits successfully, you can see a summary of the output by 
printing the model object.

```{r}
model
```

### Understanding our model

Your task is estimate the return interval for the 2013 Boulder flood. 
To do this, recall that we are defining the return interval as the inverse of the exceedance probability, or 

$$(1 - F(x; \mu, \sigma, \xi))^{-1}$$

Where does this equation come from?
Recall from our discussion above that the **exceedance probability** is the probability that a random variable exceeds some value, or in mathematical notation $\text{Pr}(X > x)$. 
The CDF is the probability that a random variable is less than or equal to some value: $\text{Pr}(X \leq x)$. 

If we think about the relationship between these two quantities, we can reason that for any particular value $x$, the probability that a random variable is less than or equal to $x$, OR greater than $x$ must be 1 (there are no other alternatives, the random variable can be less than, equal to, or greater than $x$). 
Thus, we can reason that:

$$\text{Pr}(X \leq x) + \text{Pr}(X > x) = 1$$

Recognizing that the CDF $F(x) = \text{Pr}(X \leq x)$, and subtracting $F(x)$ from both sides, notice that the exeedance probability is: 

$$\text{Pr}(X > x) = 1 - F(x)$$

Thus, the exceedance probability is the complement of the CDF (or, in English, the exceedance probability is one minus the CDF). 

#### Evaluating the GEV CDF

Your first task will be to use your estimated GEV parameters to compute the CDF at a particular value. 
There is a function called `pevd` that can evaluate the CDF of the GEV distribution, and the syntax looks like:

```{r}
pevd(1500, loc = 0, scale = 100, shape = 0, type = 'GEV')
```

Here I've plugged in arbitrary values for the parameters, but we'd like to use the estimates from our model for the location, scale, and shape parameters. 
I will pull these parameters out for you into a named vector: 

```{r}
estimates <- model$results$par
```

You can extract a specific parameter by name as follows: 

```{r}
estimates['location']
estimates['scale']
estimates['shape']
```

Evaluate the GEV CDF for the Boulder flood using these estimates, saving the value in an object called `p_2013`. 

```{r}
# Your code here
```

#### Computing a return interval from your model

Use the value of `p_2013` to compute an exceedance probability and a return interval for the 2013 Boulder flood. 

```{r}
# Your code here
```

How does your estimate compare to the media reports calling the event a "100 year" flood?

As a final check, compute the return interval for 4000 cfs. 
How does this estimate compare to your previous empirical estimate?

```{r}
# Your code here
```


## Challenges

If you have extra time, take a stab at the following challenges:

### The naming of things

Was the 2013 flood a dragon king, black swan, or perfect storm? 
Support your argument with quantitative evidence.

<!-- Your text here -->


### Pretend the year is 2012

Answer the following questions by adapting the code from above.

1. What's your estimated return interval for 3680 CFS (the discharge seen for the 2013 flood)?
2. How does this estimate compare to the estimate that we computed above?

```{r}
# Your code here
```


### Data integration

We have data from two other stream gauges. 
Can you use these data to improve your estimate of the return interval?
How would you do this?
What might be gained with such an approach, and what pitfalls might complicate your approach?
